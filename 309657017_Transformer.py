# -*- coding: utf-8 -*-
"""309657017_transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kjnIcGp824H9NAbiNr8VM8lKolOShnig
"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

import os
os.chdir('/content/drive/My Drive/hw2_309657017/')

!pip install pytorch_transformers

import seaborn as sns;
import sys
import torch
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn.functional as F
import nltk
import re
import string
from torch import nn, optim
from torch.utils import data
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import gensim
from pytorch_transformers import BertTokenizer, BertModel

class Dataset():
    def __init__(self,set_name,train):
        self.df = pd.read_csv(set_name)
        self.train_set,self.val_set = train_test_split(self.df,train_size=0.9,random_state=42)
        self.tokenizer = tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
        self.label =["business","sport","tech","politics","entertainment"]
        self.max_length = 14
        self.train = train
    def __getitem__(self,index):
        if self.train == True:
            Id,category,title = self.train_set.iloc[index].values
        else:
            Id,category,title = self.val_set.iloc[index].values

        category = self.label.index(category)
        text = self.tokenizer.tokenize(title)
        if len(text) > self.max_length-2:
            text = text[:self.max_length-2]
        text = ['[CLS]'] + text + ['[SEP]']

        ids = self.tokenizer.convert_tokens_to_ids(text)
        text_tensor = torch.tensor(ids)
        segment_tensor = torch.tensor([0]*len(ids))
        label_tensor = torch.tensor(category)
        return text_tensor,segment_tensor,label_tensor

    def __len__(self):
        if self.train:
          return len(self.train_set)
        else:
          return len(self.val_set)
    def pad_batch(self,batch):
        # collate_fn for Dataloader, pad sequence to same length and get mask tensor
        if batch[0][1] is not None:
            (text_tokens,segments,labels) = zip(*batch)
            labels = torch.stack(labels)
        else:
            (text_tokens,segments) = zip(*batch)
        texts_tokens_pad = pad_sequence(text_tokens, batch_first=True)
        segments_pad = pad_sequence(segments, batch_first=True)
        text_masks = torch.zeros(texts_tokens_pad.shape)
        text_len = [len(x) for x in text_tokens]
        for i in range(len(text_masks)):
            text_masks[i][:text_len[i]] = 1
        return texts_tokens_pad,segments_pad,text_masks,labels

class Bert_model(nn.Module):
    def __init__(self):
        super(Bert_model, self).__init__()

        self.embedding = BertModel.from_pretrained('bert-large-cased')
        self.fc = nn.Sequential(
                              nn.Linear(1024,768),
                              nn.SELU(),
                              nn.Dropout(0.1),
                              nn.Linear(768,512),
                              nn.SELU(),
                              nn.Dropout(0.1),
                              nn.Linear(512,5))

    def forward(self,ids,segments,masks,position_ids=None, head_mask=None):
        embedded = self.embedding(input_ids=ids,token_type_ids=segments,attention_mask=masks,position_ids=position_ids, head_mask=head_mask)
        cls_atten = embedded[0][:,0]  #[CLS]
        # print(embedded[0].shape)
        # print(embedded[1].shape)
        # print(cls_atten.shape)
        # sys.exit()
        head = embedded[0][0]
        logits = self.fc(cls_atten)

        return logits
def draw_chart(chart_data,outfile_name):

    plt.figure(figsize=(12.0, 6.0))
    plt.rcParams['savefig.dpi'] = 200
    plt.rcParams['figure.dpi'] = 200
    plt.plot(chart_data['epoch'],chart_data['tarin_loss'],label='tarin_loss')
    plt.grid(True,axis="y",ls='--')
    plt.legend(loc= 'best')
    plt.xlabel('epoch',fontsize=20)
    # plt.savefig('./image/'+outfile_name+'_tarin_loss.jpg')
    plt.close('all')

    plt.figure(figsize=(12.0, 6.0))
    plt.rcParams['savefig.dpi'] = 200
    plt.rcParams['figure.dpi'] = 200
    plt.plot(chart_data['epoch'],chart_data['val_acc'],label='val_acc')
    plt.plot(chart_data['epoch'],chart_data['train_acc'],label='train_acc')
    plt.grid(True,axis="y",ls='--')
    plt.legend(loc= 'best')
    plt.xlabel('epoch',fontsize=20)
    # plt.savefig('./image/'+outfile_name+'_val_acc.jpg')
    plt.close('all')

    # with open('./'+outfile_name+'.json','w') as file_object:
    #     json.dump(chart_data,file_object)
def get_acc(loader,model):
    model.eval()
    count = 0
    correct = 0
    for step, (batch) in enumerate(loader):

        ids,segments,masks,categories= [t.to(device) for t in batch]
        logits = model(ids,segments,masks)

        topv, topi = logits.topk(1)
        for predic, label in zip(topi.tolist(),categories.tolist()):
            if predic[0] == label:
                correct+=1
            count += 1

    return correct/count

if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batch_size = 256
    learning_rate = 0.001
    model_name = "Bert_model"

    train_set = Dataset("./news_data/train.csv",train=True)
    val_set = Dataset("./news_data/train.csv",train=False)

    train_loader = DataLoader(train_set,batch_size=batch_size,collate_fn=train_set.pad_batch)
    val_loader = DataLoader(val_set,batch_size=batch_size,collate_fn=val_set.pad_batch)

    model = Bert_model()
    model = model.to(device)

    optimizer = optim.SGD(model.parameters(), lr=learning_rate,weight_decay=1e-5)
    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)

    chart_data={"tarin_loss":[],"val_acc":[],"train_acc":[],"epoch":[]}
    criterion = nn.CrossEntropyLoss()

    max_acc = get_acc(val_loader,model)
    for epoch in range(200):
        train_loss = 0
        count = 0
        correct = 0
        model.train()
        optimizer.zero_grad()
        for step, (batch) in enumerate(train_loader):

            ids,segments,masks,categories= [t.to(device) for t in batch]
            logits = model(ids,segments,masks)

            topv, topi = logits.topk(1)
            for predic, label in zip(topi.tolist(),categories.tolist()):
                if predic[0] == label:
                    correct+=1
                count += 1

            model.zero_grad()
            loss = criterion(logits, categories)
            train_loss += loss.item() #take out the loss number
            loss.backward()
            optimizer.step()
            # scheduler.step()

        v_acc = get_acc(val_loader,model)
        if v_acc > max_acc :
            max_acc = v_acc
            torch.save(model.state_dict(), './news_data/'+model_name+'.pkl')
        print('Epoch: ' , str(epoch) , \
                '\ttrain_loss: '+str(round(train_loss/(step+1),5)),\
                '\ttrain_acc: '+str(round(correct/count,4)), \
                '\tval_acc: '+str(round(v_acc,4))
            )
        chart_data['epoch'].append(epoch)
        chart_data['tarin_loss'].append(train_loss/(step+1))
        chart_data['train_acc'].append(correct/count)
        chart_data['val_acc'].append(v_acc)

draw_chart(chart_data,model_name)

class hw1_Dataset(Dataset):
    def __init__(self,set_name):
        self.df = pd.read_csv(set_name)
        self.tokenizer = tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
        self.label =["business","sport","tech","politics","entertainment"]
        self.max_length = 14
    def __getitem__(self,index):
        Id,title = self.df.iloc[index].values
        text = self.tokenizer.tokenize(title)
        if len(text) > self.max_length-2:
            text = text[:self.max_length-2]
        text = ['[CLS]'] + text + ['[SEP]']

        ids = self.tokenizer.convert_tokens_to_ids(text)
        text_tensor = torch.tensor(ids)
        segment_tensor = torch.tensor([0]*len(ids))
        return text_tensor,segment_tensor
    def __len__(self):
        return len(self.df)
    def pad_batch(self,batch):
        # collate_fn for Dataloader, pad sequence to same length and get mask tensor
        (text_tokens,segments) = zip(*batch)
        texts_tokens_pad = pad_sequence(text_tokens, batch_first=True)
        segments_pad = pad_sequence(segments, batch_first=True)
        text_masks = torch.zeros(texts_tokens_pad.shape)
        text_len = [len(x) for x in text_tokens]
        for i in range(len(text_masks)):
            text_masks[i][:text_len[i]] = 1
        return texts_tokens_pad,segments_pad,text_masks

if __name__ == '__main__':
    # -------------- hyper parameter --------------
    model_name = 'Bert_model'
    # ------------------------------------------
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    test_set = hw1_Dataset("./news_data/test.csv")
    test_loader = DataLoader(test_set,batch_size=1,collate_fn=test_set.pad_batch)
    predict = []
    with torch.no_grad():
      model = Bert_model() 
      model = model.to(device)   
      model.load_state_dict(torch.load('./news_data/'+model_name+'.pkl'))
      model.eval()
      for step, (batch) in enumerate(test_loader):
          ids,segments,masks = [t.to(device) for t in batch]
          logits = model(ids,segments,masks)
          topv, topi = logits.topk(1)
          predict.append(topi.tolist()[0][0])
    label = []
    for i in predict:
      if i == 0:
        label.append("business")
      elif i ==1:
        label.append("sport")
      elif i==2:
        label.append("tech")
      elif i==3:
        label.append("politics")
      else:
        label.append("entertainment")
    print(label)

test_set.df.iloc[:,0] = np.array(list(label))
test_set.df.iloc[:,0].to_csv('309657017_submission_transformer.csv',index_label="Id",header=["Category"])

