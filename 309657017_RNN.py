# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iv7QryXzoamIxt5ZkoQmo4eMNmu5yJw_
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/hw2_309657017/')

import  nltk
nltk.download('punkt')

nltk.download('stopwords')

import sys
import torch
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn.functional as F
import string
import re
from torch import nn, optim
from torch.utils import data
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords
import gensim

class Dataset():
    def __init__(self,set_name,vocab,train=True):
        self.df = pd.read_csv(set_name)
        self.train_,self.val_ = train_test_split(self.df,train_size=0.8,random_state=42)
        self.label = ["business","sport","tech","politics","entertainment"]
        self.train = train
        self.vocab = vocab
    def __getitem__(self,index): 
        if self.train:
            Id,category,title = self.train_.iloc[index].values
        else:
            Id,title = self.val_.iloc[index].values
        
        category = self.label.index(category)
        
        title = re.sub(r'\[[0-9]*\]',' ',title)
        title = re.sub(r'\d+',' ',title)
        title = re.sub(r'\s+',' ',title)
        title = title.lower()
        title = "".join([char for char in title if char not in string.punctuation])
        
        title = nltk.word_tokenize(title)
        text = []
        for i in title:
            if i not in stopwords.words('english'):
                text.append(i)
        ids = []
        for i in text :
            if i in self.vocab:
                ids.append(self.vocab[i])
            else:
                ids.append(1)
        text_tensor = torch.tensor(ids)
        text_len_tensor = torch.tensor(len(text_tensor))
        label_tensor = torch.tensor(category)
        return text_tensor,label_tensor,text_len_tensor

    def __len__(self):
        if self.train:
          return len(self.train_)
        else:
          return len(self.val_)

    def pad_batch(self,batch):
        # collate_fn for Dataloader, pad sequence to same length and get mask tensor
        if batch[0][1] is not None:
            (text_tokens,labels,textlens) = zip(*batch)
            labels = torch.stack(labels)
            lens = torch.stack(textlens)
        else:
            (text_tokens,textlens) = zip(*batch)
        texts_tokens_pad = pad_sequence(text_tokens, batch_first=True)
        return texts_tokens_pad,labels,lens

class RNN_model(nn.Module):
    def __init__(self,w2v):
        super(RNN_model, self).__init__()
        # vectors = np.concatenate((np.zeros((2,300),dtype=np.float32), v), axis=0)
        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(w2v))
        self.embedding.weight.requires_grad = False
        self.lstm = nn.LSTM(100, 512, 
                           num_layers=2,
                           bidirectional=True,
                           dropout=0,
                           batch_first=True)
        self.fc = nn.Sequential(
                            nn.Linear(1024,512),
                            nn.Tanh(),
                            nn.Linear(512,256),
                            nn.Tanh(),
                            nn.Linear(256,5)
                        )
    def forward(self, text,text_lengths): # text = [batch size,sent_length]
        embedded = self.embedding(text) # embedded = [batch size, sent_len, emb dim]
        # packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True,enforce_sorted=False)
        packed_output, (hidden,cell) = self.lstm(embedded) # hidden = [num_layers * num_directions, batch, hidden_size]
        # ------ concat the final forward and backward hidden state ------
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)   # hidden = [batch size, hid dim * num directions] 
        logits = self.fc(hidden)       
        return logits

def load_glove_model(glove_file):
    embeddings_index = {}
    f = open(glove_file, encoding='utf8')
    for line in f:
        values = line.split()

        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()
    word_map = {
    word:idx
    for idx,word in enumerate(embeddings_index, start=2)
    }
    embedding_matrix = np.zeros((len(word_map)+2, 100))
    for index,(word, i) in enumerate(embeddings_index.items()):
        embedding_matrix[index+2] = embeddings_index.get(word)
    return embedding_matrix,word_map

def get_acc(loader,model,epoch):
    model.eval()
    count = 0
    correct = 0
    for step, (batch) in enumerate(loader):
        titles,categories,text_lengths = [t.to(device) for t in batch]
        logits = model(titles,text_lengths.tolist())
        topv, topi = logits.topk(1)
        for predic, label in zip(topi.tolist(),categories.tolist()):
            if predic[0] == label:
                correct+=1
            count += 1

        # break
    return correct/count
def draw_chart(chart_data,outfile_name):
    # -------------- draw loss image --------------
    # -------------- new one figure and set resolution --------------
    plt.figure(figsize=(12.0, 6.0))
    plt.rcParams['savefig.dpi'] = 200
    plt.rcParams['figure.dpi'] = 200
    # -------------- plot data in image --------------
    plt.plot(chart_data['epoch'],chart_data['tarin_loss'],label='tarin_loss')
    # -------------- draw underline --------------
    plt.grid(True,axis="y",ls='--')
    # -------------- draw legent --------------
    plt.legend(loc= 'best')
    # -------------- show lable --------------
    plt.xlabel('epoch',fontsize=20)
    # plt.yticks(np.linspace(0,1,11))
    # plt.savefig('./'+outfile_name+'_tarin_loss.jpg')
    plt.close('all')
    # --------------

    plt.figure(figsize=(12.0, 6.0))
    plt.rcParams['savefig.dpi'] = 200
    plt.rcParams['figure.dpi'] = 200
    plt.plot(chart_data['epoch'],chart_data['val_acc'],label='val_acc')
    plt.plot(chart_data['epoch'],chart_data['train_acc'],label='train_acc')
    plt.grid(True,axis="y",ls='--')
    plt.legend(loc= 'best')
    plt.xlabel('epoch',fontsize=20)
    # plt.yticks(np.linspace(0,1,11))
    # plt.savefig('./'+outfile_name+'_val_acc.jpg')
    # with open('./'+outfile_name+'.json','w') as file_object:
    #     json.dump(chart_data,file_object)

if __name__ == '__main__':
    embedding_matrix,id = load_glove_model('./news_data/glove.6B.100d.txt')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batch_size = 256
    learning_rate = 1
    model_name = "RNN_model3"

    train_set = Dataset("./news_data/train.csv",id,train=True)
    train_loader = DataLoader(train_set,batch_size=batch_size,collate_fn=train_set.pad_batch)
    test_set = Dataset("./news_data/train.csv",id,train=False)
    test_loader = DataLoader(test_set,batch_size=1,collate_fn=test_set.pad_batch)
    model = RNN_model(embedding_matrix)
    model = model.to(device)
    # del(w2v)
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.9)

    chart_data={"tarin_loss":[0],"val_acc":[0],"train_acc":[0],"epoch":[0]}
    criterion = nn.CrossEntropyLoss()
    max_acc = get_acc(test_loader,model,-1)
    for epoch in range(150):
        train_loss = 0
        count = 0
        correct = 0
        model.train()
        optimizer.zero_grad()
        for step, (batch) in enumerate(train_loader):
            titles,categories,text_lengths = [t.to(device) for t in batch]
            logits = model(titles,text_lengths.tolist())
            topv, topi = logits.topk(1)
            for predic, label in zip(topi.tolist(),categories.tolist()):
                if predic[0] == label:
                    correct+=1
                count += 1

            model.zero_grad()
            loss = criterion(logits, categories)
            train_loss += loss.item() #take out the loss number
            loss.backward()
            optimizer.step()
            scheduler.step()
        v_acc= get_acc(test_loader,model,epoch)
        if v_acc > max_acc :
            max_acc = v_acc
            torch.save(model.state_dict(), './news_data/'+model_name+'.pkl')

        print('Epoch: ' , str(epoch) , \
                '\ttrain_loss: '+str(round(train_loss/(step+1),5)),\
                '\ttrain_acc: '+str(round(correct/count,4)) ,\
                '\tval_acc: '+str(round(v_acc,4)) \
            )
        chart_data['epoch'].append(epoch)
        chart_data['tarin_loss'].append(train_loss/(step+1))
        chart_data['train_acc'].append(correct/count)
        chart_data['val_acc'].append(v_acc)
draw_chart(chart_data,model_name)

class hw1_Dataset(Dataset):
    def __init__(self,set_name,vocab):
        self.df = pd.read_csv(set_name)
        self.label = ["business","sport","tech","politics","entertainment"]
        self.vocab = vocab

    def __getitem__(self,index): 

        Id,title = self.df.iloc[index].values
        title = re.sub(r'\[[0-9]*\]',' ',title)
        title = re.sub(r'\d+',' ',title)
        title = re.sub(r'\s+',' ',title)
        title = title.lower()
        title = "".join([char for char in title if char not in string.punctuation])
        
        title = nltk.word_tokenize(title)
        text = []
        for i in title:
            if i not in stopwords.words('english'):
                text.append(i)
        ids = []
        for i in text :
            if i in self.vocab:
                ids.append(self.vocab[i])
            else:
                ids.append(1)
        text_tensor = torch.tensor(ids)
        text_len_tensor = torch.tensor(len(text_tensor))
        return text_tensor,text_len_tensor

    def __len__(self):
        return len(self.df)

    def pad_batch(self,batch):
        (text_tokens , lens) = zip(*batch)
        lens = torch.stack(lens)
        texts_tokens_pad = pad_sequence(text_tokens, batch_first=True)
        return texts_tokens_pad,lens
if __name__ == '__main__':
    # -------------- hyper parameter --------------
    model_name = 'RNN_model3'
    # ------------------------------------------
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    test_set = hw1_Dataset("./news_data/test.csv",id)
    test_loader = DataLoader(test_set,batch_size=1,collate_fn=test_set.pad_batch)
    predict = []
    with torch.no_grad():
      model = RNN_model(embedding_matrix) 
      model = model.to(device)   
      model.load_state_dict(torch.load('./news_data/'+model_name+'.pkl'))
      model.eval()
      for step, (batch) in enumerate(test_loader):
          titles,lens = [t.to(device) for t in batch]
          logits = model(titles,lens)
          topv, topi = logits.topk(1)
          predict.append(topi.tolist()[0][0])
    print(predict)
    label = []
    for i in predict:
      if i == 0:
        label.append("business")
      elif i ==1:
        label.append("sport")
      elif i==2:
        label.append("tech")
      elif i==3:
        label.append("politics")
      else:
        label.append("entertainment")
    print(label)

test_set.df.iloc[:,0] = np.array(list(label))
test_set.df.iloc[:,0].to_csv('309657017_submission_RNN.csv',index_label="Id",header=["Category"])

